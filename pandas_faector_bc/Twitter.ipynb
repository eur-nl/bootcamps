{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Stream with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook connects to a (filtered) real-time Twitter stream.  \n",
    "Incoming tweets are classified on sentiment, either positive or negative.  \n",
    "We'll use a Naive Bayes analyzer (pre-trained on movie reviews).  \n",
    "And we'll train a classifier ourselfs, based on human classified tweets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, you need to get credentials from Twitter.  \n",
    "Acquiring credentials is described here : https://www.slickremix.com/docs/how-to-get-api-keys-and-tokens-for-twitter/  \n",
    "Put the credentials in a file called ```twitter_credentials.py``` in the same folder as this notebook.  \n",
    "The format of that file needs to be :  \n",
    "\n",
    "```\n",
    "consumer_key = \"THE_ACTUAL_CONSUMER_KEY\"\n",
    "consumer_secret = \"THE_ACTUAL_CONSUMER_SECRET\"\n",
    "access_token = \"THE_ACTUAL_ACCESS_TOKEN\"\n",
    "access_token_secret = \"THE_ACTUAL_ACCESS_TOKEN_SECRET\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Conda installation might not have all the necessary modules.  \n",
    "Run the commands below once by uncommenting them, and running the cell.  \n",
    "Afterwards comment the lines to improve run speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "#!pip install tweepy\n",
    "#!pip install textblob\n",
    "#!pip install nltk\n",
    "\n",
    "# If needed\n",
    "# import nltk\n",
    "# nltk.download()  # Select twitter_samples under tab 'Corpora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports always goes on top\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.corpus import twitter_samples\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Import the credentials\n",
    "import twitter_credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "Create the authentication object, only needed once per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(twitter_credentials.consumer_key, twitter_credentials.consumer_secret)\n",
    "auth.set_access_token(twitter_credentials.access_token, twitter_credentials.access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We start by training a classifier with the NLTK module, with the provided samples.  \n",
    "These samples are classified by humans on positive or negative sentiment.  \n",
    "This might take some time, so only do this once per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of 2-tuples, with each 2-tuple a list of strings and a label  \n",
    "train = []\n",
    "\n",
    "# First add the negative tweets\n",
    "for tokens in twitter_samples.tokenized('negative_tweets.json'):\n",
    "    train.append((tokens, 'neg'))\n",
    "    \n",
    "# Then add the positive tweets\n",
    "for tokens in twitter_samples.tokenized('positive_tweets.json'):\n",
    "    train.append((tokens, 'pos'))\n",
    "\n",
    "# Shuffle and take a subset, this speeds op speed up training\n",
    "random.shuffle(train)\n",
    "train = train[0:600]\n",
    "\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet class\n",
    "\n",
    "Let's define a class that receives the tweet, and classifies the text.  \n",
    "Also define methods to print the sentiment, language and the tweet itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    \"\"\"This class creates a tweet from a JSON string\"\"\"\n",
    "    def __init__(self, data, cl):\n",
    "        # Hint : print(self._tweet.keys()) for all keys in the tweet\n",
    "        self._tweet = json.loads(data)\n",
    "        self.blob1 = TextBlob(self._tweet[\"text\"], classifier=cl)\n",
    "        self.blob2 = TextBlob(self._tweet[\"text\"], analyzer=NaiveBayesAnalyzer())\n",
    "        \n",
    "    def print_tweet(self):\n",
    "        print()\n",
    "        print(\"-\" * 80)\n",
    "        print(self._tweet[\"id_str\"], self._tweet[\"created_at\"])\n",
    "        print(self._tweet[\"text\"])\n",
    "        \n",
    "    def print_language(self):\n",
    "        print(\"language\", self.blob1.detect_language())\n",
    "        \n",
    "    def print_sentiment(self):\n",
    "        print(\"sentiment\", self.blob1.classify())\n",
    "        print(self.blob2.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listener class\n",
    "\n",
    "Here we define a listener, that processes the stream.  \n",
    "If it receives data, create a Tweet object and classifies the tweet.  \n",
    "It also prints the various characteristics and checks if it needs to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    "    \"\"\"Listener class that processes a Twitter Stream\"\"\"\n",
    "    def __init__(self, max_count, cl):\n",
    "        self.max_count = max_count\n",
    "        self.count = 0\n",
    "        self.cl = cl\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        self.tweet = Tweet(data, cl)\n",
    "        self.tweet.print_tweet()\n",
    "        self.tweet.print_language()\n",
    "        self.tweet.print_sentiment()\n",
    "                \n",
    "        self.count += 1\n",
    "        if self.count >= self.max_count:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "First instantiate a listener from the definition that stops after 10 tweets.  \n",
    "We pass it our trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylistener = MyListener(10, cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the listener to the stream, pass the authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystream = Stream(auth, listener=mylistener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of keywords to filter the stream of new tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Econometrics', 'Operations Research', 'Erasmus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the keywords, start the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mystream.filter(track = keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're done, disconnect from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
