{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textmining with NLTK\n",
    "\n",
    "NLTK (Natural Language Tool Kit) and Spacy are two Python libraries often used for textual analysis. NLTK has been around for quite some time. It is a huge library, with many of it's use-cases well documented.\n",
    "\n",
    "Spacy is the \"new kid on the block\", documentation is not as thorough as NLTK, but it's website offers code examples and some good tutorials can be found online. Seems to lean a litlle bit more towards easy integration of machine learning somewhere in the pipeline.\n",
    "\n",
    "For most code examples we will use NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The import above is too long to type repeatedly, so we use another way of importing the textual sources\n",
    "from nltk.corpus import gutenberg as gb\n",
    "gb.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with NLTK and it's notion of a corpus, we are now quite flexible as to how precisely solve our problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we can decide if we want to work with one text, a couple of texts or all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to work with all of the texts shown above, we would wrap our code in a for-loop:\n",
    "print(\"word sent vocab\\tFILE\\n\")\n",
    "for fileid in gb.fileids():\n",
    "    num_chars = len(gb.raw(fileid))\n",
    "    num_words = len(gb.words(fileid))\n",
    "    num_sents = len(gb.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gb.words(fileid)))\n",
    "    print(round(num_chars/num_words),  # Average word length\n",
    "          round(num_words/num_sents),  # Average sentence length\n",
    "          round(num_words/num_vocab),  # On average the number of times each vocab item appears in the text\n",
    "          fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can also choose to start with one text file:\n",
    "emma = gb.raw('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the sents method directly on the Gutenberg corpus with the fileid of the text we are interested in.\n",
    "tokenized_emma_sents = gb.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_emma_sents[0:5]:\n",
    "    print('Sentence: ')\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the raw textfiles from the Gutenberg collection as our textual sources, but the good news is that the NLTK library provides the methods to quickstart with your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup could be as follows:\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "CORPUS_ROOT = \"/Users/peter/Documents/repub/wip/Samare/data/testing\"\n",
    "\n",
    "contents = PlaintextCorpusReader(CORPUS_ROOT, '.*\\.txt')\n",
    "for fileid in contents.fileids():\n",
    "    # Do something useful\n",
    "    print(fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A spacy example (we can use the NLTK groundwork)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\"\"\"\n",
    "A lot is happening after we declared the previous two statements:\n",
    "- we told Spacy that we are going to use the language \"en\" or English\n",
    "- and we will use the pipeline [\"tagger\", \"parser\", \"ner\"]\n",
    "\"\"\"\n",
    "\n",
    "blake = gb.raw('blake-poems.txt')\n",
    "blake_process = blake[:100]\n",
    "doc = nlp(blake_process)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy comes with a lot of batteries included (pipelines, word vectors, etc.). If you are into hard-core text and data mining it might be worthwhile to dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on with something more basic: Get rid of things we do not need when working with texts, like stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to \\\n",
    "        unite some of the best blessings of existence; and had lived nearly twenty-one years in the world \\\n",
    "        with very little to distress or vex her.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "content_tokens = [token for token in tokens if token.lower() not in english_stopwords]\n",
    "print(content_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can go into several directions, which direction is best depends on the problem(s) we want to solve.\n",
    "\n",
    "- We could analyse the narrative of a novel, plotting the appearance of main characters in chapters.\n",
    "- We could construct so-called \"synopsis documents\" using certain characteristics of the words used in documents: freq. used terms, freq used bi- and trigrams, hapax's, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist()\n",
    "emma_words = gb.words('austen-emma.txt')\n",
    "print(len(emma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the usual pre-processing:\n",
    "e_words = [word for word in emma_words if not word.isnumeric()]\n",
    "e_words = [word for word in e_words if word.isalpha()]\n",
    "e_words = [word for word in e_words if len(word) > 2]\n",
    "e_words = [word.lower() for word in e_words if word not in english_stopwords]\n",
    "# we could add a filter for stemming words, but ...\n",
    "len(e_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the 25 most used words in Jane Austen's \"Emma\".\n",
    "fdist = nltk.FreqDist(e_words)\n",
    "for word, frequency in fdist.most_common(25):\n",
    "    print('{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.hapaxes()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fish for all bigrams very easily with the bigram method, but note that we just get the bigrams word order\n",
    "# with one word overlap\n",
    "list(nltk.bigrams(e_words))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the same goes for naive trigram fishing:\n",
    "list(nltk.trigrams(e_words))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do better:\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(e_words)\n",
    "bigram_finder.apply_freq_filter(10)\n",
    "print(bigram_finder.nbest(bigram_measures.pmi, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_words(e_words)\n",
    "trigram_finder.apply_freq_filter(10)\n",
    "print(trigram_finder.nbest(trigram_measures.pmi, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we the PMI measure to filter out the most frequently used bi- and trigrams, we can use tf-idf to the most frequent terms used in a document set against the background of inversed document frequency within a corpus.\n",
    "\n",
    "For this we need to import another library: gensim.\n",
    "\n",
    "Fot the following examples the scenario is that we will use tf-idf to characterize docs within a corpus; modelling stuff. Whenever we come across a new doc we can scan if we have \"similar\" docs.\n",
    "\n",
    "Tf-idf works ok if we are dealing with not so large corpora. Word2vec and Doc2vec kick in when things get larger and larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "raw_documents = [gb.raw('austen-emma.txt'), gb.raw('blake-poems.txt'), gb.raw('milton-paradise.txt')]\n",
    "def get_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [get_tokens(text) for text in raw_documents]\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "num_words = len(dictionary)\n",
    "print(\"Number of words in dictionary: {}\".format(num_words))\n",
    "#for idx,word in dictionary.items():\n",
    "    #print(idx,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have quick look:\n",
    "print(dictionary[18])\n",
    "print(dictionary.id2token[18])\n",
    "print(dictionary.token2id['comfortable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc = dictionary.doc2bow(['I', 'love', 'tacos'])\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.id2token[951])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a corpus: a list of bags of words\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the corpus to create a tf-idf model (num_nnz is the number of tokens)\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a closer look at what we have:\n",
    "# print(gen_docs[0])\n",
    "# print(corpus[0])\n",
    "# print(tf_idf[corpus][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so far so good\n",
    "# Let's take a new document and process that\n",
    "bow = dictionary.doc2bow(get_tokens(gb.raw('austen-sense.txt')))\n",
    "query_doc_tf_idf = tf_idf[bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('/Users/peter/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes next?\n",
    "\n",
    "Doc2vec which performs really well with larger corpora (word2vec and doc2vec are big in the ML environments of unsupervised learning). Tf-idf scores good when the amount of data is smaller.\n",
    "\n",
    "And then there are of course strategies like: Classification of documents with the computer, clustering, topic maps, etc., etc.\n",
    "\n",
    "As always, one should use techniques and libraries that are sound solutions for the problems one wants to solve. making things very complicated is the easiest thing to do, but elegant and simple solutions to the problems at hand are the goals here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
